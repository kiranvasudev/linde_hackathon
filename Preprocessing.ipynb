{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.layouts import column\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 6000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colocations = pd.read_csv(\"../data/colocations.csv\", index_col=0)\n",
    "customer_details = pd.read_csv(\"../data/customer_details.csv\", index_col=0)\n",
    "deliveries = pd.read_csv(\"../data/deliveries.csv\", index_col=0)\n",
    "level_readings = pd.read_csv(\"/data/Linde_Intel_AI_Challenge_Nov2018/level_readings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to date and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deliveries[\"del_date\"] = pd.to_datetime(deliveries[\"DELIVERY_DATE\"]).dt.date\n",
    "deliveries[\"del_time\"] = pd.to_datetime(deliveries[\"DELIVERY_DATE\"]).dt.time\n",
    "deliveries[\"DELIVERY_DATE\"] = pd.to_datetime(deliveries[\"DELIVERY_DATE\"])\n",
    "\n",
    "level_readings[\"read_date\"] = pd.to_datetime(level_readings[\"ON_DATE_TIME\"]).dt.date\n",
    "level_readings[\"read_time\"] = pd.to_datetime(level_readings[\"ON_DATE_TIME\"]).dt.time\n",
    "level_readings[\"ON_DATE_TIME\"] = pd.to_datetime(level_readings[\"ON_DATE_TIME\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ... | ... | ... |\n",
    "| --- | --- | --- |\n",
    "|VESSEL_ID 1 |Identifier of primary storage vessel |VARCHAR|\n",
    "|VESSEL_ID 2 - 7 |Identifier of any (up to seven) co-located storage vessels |VARCHAR|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colocations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer deets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ... | ... | ...|\n",
    "| --- | --- | --- |\n",
    "|INST_ID                     | Installation Identifier                                         | INT      |\n",
    "|CNTRY_UN_COUNTRY_CODE b     | Two-digit standard country identifier                           | CHAR (2) |\n",
    "|PROVINCE                    |  Geographic location Province/State/County                      | VARCHAR  |\n",
    "|PRD_ID                      | Abbreviation of Product Name                                    | CHAR (3) |\n",
    "|MAXIMUM_PRODUCT_CAPACITY    | Maximum capacity of the storage vessel                          | INT (KG) |\n",
    "|VESSEL_ID                   | Concatenation of Country identifier and Installation identifier | VARCHAR  |\n",
    "|MKT_SEGMENT                 | Market Segment Classification                                   | VARCHAR  |\n",
    "|ISIC_CODE (where available) |  International Standard Industrial Classification (ISIC) code   | INT      |\n",
    "|ISIC_DESC                   | ISIC Code Description                                           | VARCHAR  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_details.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliveries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|...|...|...|\n",
    "|--- | ---| ---|\n",
    "|INST_ID |Installation Identifier |INT\n",
    "|DELIVERY_DATE |Date/time of delivery |YYYY-MM-DD HH:MM\n",
    "|DELIVERED_VOLUME | Amount of product delivered |INT (KG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deliveries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level readings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|...|...|...|\n",
    "|--- | ---| ---|\n",
    "|INST_ID |Installation Identifier| INT\n",
    "|ON_DATE_TIME| Date/time of reading| YYYY-MM-DD HH:MM\n",
    "|INST_PRODUCT_AMOUNT |Instantaneous product level reading | INT (KG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "level_readings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a new feature called sensor status. \n",
    "\n",
    "### Assigns 0 if 4 consecutive readings are 0 when there is also a delivery otherwise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delivery_occurred(data, start_date, end_date):\n",
    "    \n",
    "    # get all data between start and end dates\n",
    "    \n",
    "    indices_above = np.where(deliveries[\"DELIVERY_DATE\"] >= start_date)[0]\n",
    "    indices_below = np.where(deliveries[\"DELIVERY_DATE\"] < end_date)[0]\n",
    "    indices_within = np.intersect1d(indices_above, indices_below)\n",
    "    \n",
    "    if sum(deliveries.loc[indices_within, \"DELIVERED_VOLUME\"]) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_status = np.ones(level_readings.shape[0])\n",
    "failure_indices=[]\n",
    "\n",
    "for ves_id, vessel_group in level_readings.groupby(\"VESSEL_ID\"):\n",
    "    zero_indices = vessel_group.index[np.where(vessel_group[\"INST_PRODUCT_AMOUNT\"].values == 0)[0]]\n",
    "    \n",
    "    for k, g in groupby(enumerate(zero_indices), lambda x:x[0]-x[1]):\n",
    "        group = list(map(itemgetter(1), g)) # group of continuous indices\n",
    "        if len(group) > 4:\n",
    "            for index in range(len(group)):\n",
    "                if index + 4 < len(group):\n",
    "#                     display(vessel_group.loc[group[index: index + 4], \"ON_DATE_TIME\"])\n",
    "                    \n",
    "                    if delivery_occurred(deliveries, vessel_group.loc[group[index], \"ON_DATE_TIME\"], vessel_group.loc[group[index+4], \"ON_DATE_TIME\"]):\n",
    "#                         _status[group[index] : group[index+4]] = 0\n",
    "                        failure_indices.append(group[index:index+4])\n",
    "            \n",
    "_status[failure_indices] = 0\n",
    "\n",
    "level_readings[\"sensor_status\"] = _status\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dealing with fake 0s (timestamps with value 0) - process repeated until there are no fake 0s anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    old_shape = level_readings.shape[0]\n",
    "    indices_to_remove = []\n",
    "    # group by vessel id\n",
    "    for ves_id, vessel_group in level_readings.groupby(\"VESSEL_ID\"):\n",
    "\n",
    "        # sort the group\n",
    "        sorted_vessel_group = vessel_group.sort_values(by=[\"ON_DATE_TIME\"])\n",
    "\n",
    "        # find max and min timestamps in the group\n",
    "        sorted_max = sorted_vessel_group[\"ON_DATE_TIME\"].max()\n",
    "        sorted_min = sorted_vessel_group[\"ON_DATE_TIME\"].min()\n",
    "\n",
    "    #     print(sorted_vessel_group.dtypes)\n",
    "    #     print(sorted_vessel_group[\"ON_DATE_TIME\"].dt.minute.value_counts())\n",
    "\n",
    "        zero_indices = sorted_vessel_group.index[np.where(sorted_vessel_group[\"ON_DATE_TIME\"].dt.minute == 0)[0]]\n",
    "        zero_indices_1 = zero_indices + 1\n",
    "        zero_indices_0 = zero_indices - 1\n",
    "        \n",
    "        one_indices = sorted_vessel_group.index[np.where(sorted_vessel_group[\"ON_DATE_TIME\"].dt.minute == 1)[0]]\n",
    "        one_indices_1 = one_indices + 1\n",
    "        one_indices_0 = one_indices - 1\n",
    "\n",
    "    #     print(sorted_vessel_group.loc[zero_indices, \"ON_DATE_TIME\"].dt.minute.value_counts())\n",
    "    #     print(sorted_vessel_group.loc[zero_indices_1, \"ON_DATE_TIME\"].dt.minute.value_counts())\n",
    "    #     print(sorted_vessel_group.loc[zero_indices_0, \"ON_DATE_TIME\"].dt.minute.value_counts())\n",
    "\n",
    "        # check where the plus one index and minus one index have timestamps with minute as 59\n",
    "        # this means that the 0 value is wrong, hence delete\n",
    "        plus_one_indices = sorted_vessel_group.loc[zero_indices_1][sorted_vessel_group.loc[zero_indices_1, \"ON_DATE_TIME\"].dt.minute == 59].index.values - 1\n",
    "        minus_one_indices = sorted_vessel_group.loc[zero_indices_0][sorted_vessel_group.loc[zero_indices_0, \"ON_DATE_TIME\"].dt.minute == 59].index.values + 1\n",
    "        \n",
    "        # for the ones\n",
    "        plus_one_indices_one = sorted_vessel_group.loc[one_indices_1][sorted_vessel_group.loc[one_indices_1, \"ON_DATE_TIME\"].dt.minute == 59].index.values - 1\n",
    "        minus_one_indices_one = sorted_vessel_group.loc[one_indices_0][sorted_vessel_group.loc[one_indices_0, \"ON_DATE_TIME\"].dt.minute == 59].index.values + 1\n",
    "        \n",
    "        int_one = np.intersect1d(plus_one_indices_one, minus_one_indices_one).tolist()\n",
    "        int_zero = np.intersect1d(plus_one_indices, minus_one_indices).tolist()\n",
    "        \n",
    "        indices_to_remove.append(np.intersect1d(int_one, int_zero).tolist())\n",
    "#         indices_to_remove.append(int_one)\n",
    "\n",
    "    if len(indices_to_remove) > 0:\n",
    "        level_readings.drop(index=sum(indices_to_remove,[]), inplace=True)\n",
    "        level_readings.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if old_shape == level_readings.shape[0]:\n",
    "        break\n",
    "    \n",
    "#     print(plus_one_indices)\n",
    "#     indices_within = np.where()\n",
    "    \n",
    "#     break\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after_prep = level_readings.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with sharp changes in level readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to check if there is no delivery and also if the level increases sharply. \n",
    "# then find the mean of the previous and next point\n",
    "# level_readings = after_prep.copy()\n",
    "\n",
    "for ves_id, vessel_group in level_readings.groupby(\"VESSEL_ID\"):\n",
    "    sorted_vessel_group = vessel_group.sort_values(by=[\"ON_DATE_TIME\"])\n",
    "    sorted_delivery_group = deliveries[deliveries[\"VESSEL_ID\"] == ves_id].sort_values(by=[\"DELIVERY_DATE\"])\n",
    "    \n",
    "    data_x = sorted_vessel_group[\"INST_PRODUCT_AMOUNT\"].values\n",
    "\n",
    "    data_x_plus = np.zeros(data_x.shape)\n",
    "    data_x_plus[:-1] = data_x[1:]\n",
    "    \n",
    "    data_x_minus = np.zeros(data_x.shape)\n",
    "    data_x_minus[1:] = data_x[1:]\n",
    "  \n",
    "    dist_x_minus_1 = abs(data_x - data_x_minus)\n",
    "    dist_x_plus_1 = abs(data_x_plus - data_x)\n",
    "\n",
    "    indices_minus = np.where(dist_x_minus_1 > 100)[0]\n",
    "    indices_plus = np.where(dist_x_plus_1 > 100)[0]\n",
    "    \n",
    "    final_indices = sorted_vessel_group.index[np.unique(np.concatenate((indices_minus+1, indices_plus-1)))]\n",
    "    final_indices_minus = final_indices - 1\n",
    "    final_indices_plus = final_indices + 1\n",
    "\n",
    "    values = (sorted_vessel_group.loc[final_indices_minus, \"INST_PRODUCT_AMOUNT\"].values + \\\n",
    "         sorted_vessel_group.loc[final_indices_plus, \"INST_PRODUCT_AMOUNT\"].values) / 2\n",
    "    \n",
    "    nan_indices = np.where(pd.isnull(values) == True)[0]\n",
    "    \n",
    "    if len(nan_indices) > 0:\n",
    "#         print(nan_indices)\n",
    "        values[nan_indices] = values[nan_indices] - 2\n",
    "    \n",
    "    level_readings.loc[final_indices, \"INST_PRODUCT_AMOUNT\"] = values\n",
    "    \n",
    "#     print(data_x.shape)\n",
    "    \n",
    "#     print(final_indices_minus.values)\n",
    "#     print(final_indices_plus.values)\n",
    "    \n",
    "#     sorted_vessel_group.loc[final_indices, \"INST_PRODUCT_AMOUNT\"] = \n",
    "#     print(sorted_vessel_group.loc[final_indices_minus, \"INST_PRODUCT_AMOUNT\"].values)\n",
    "#     print()\n",
    "#     print(sorted_vessel_group.loc[final_indices_minus, \"INST_PRODUCT_AMOUNT\"].values)\n",
    "#     print()\n",
    "#     print(type(sorted_vessel_group.loc[final_indices_minus, \"INST_PRODUCT_AMOUNT\"].values[0]))\n",
    "#     print(sorted_vessel_group.loc[final_indices_minus, \"INST_PRODUCT_AMOUNT\"].values + \\\n",
    "#          sorted_vessel_group.loc[final_indices_plus, \"INST_PRODUCT_AMOUNT\"].values)\n",
    "#     print()   \n",
    "#     print(\"ori last\", data_x[-1])\n",
    "#     print(\"ori second last\", data_x[-2])\n",
    "#     print(\"ori first\", data_x[0])\n",
    "#     print(\"ori second\", data_x[1])\n",
    "#     print(\"minus last\", data_x_minus[-1])\n",
    "#     print(\"minus first\", data_x_minus[0])\n",
    "#     print(\"minus second\", data_x_minus[0])\n",
    "#     print(\"plus second last\", data_x_plus[-2])\n",
    "#     print(\"plus last\", data_x_plus[-1])\n",
    "#     print(\"plus first\", data_x_plus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_readings.to_csv(\"processed_level.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the processed csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_readings = pd.read_csv(\"processed_level.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the timestamps to appropriate format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_readings[\"read_date\"] = pd.to_datetime(level_readings[\"ON_DATE_TIME\"]).dt.date\n",
    "level_readings[\"read_time\"] = pd.to_datetime(level_readings[\"ON_DATE_TIME\"]).dt.time\n",
    "level_readings[\"ON_DATE_TIME\"] = pd.to_datetime(level_readings[\"ON_DATE_TIME\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to clip all values above the max capacity of the vessel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# find all points where the level is higher than the max capacity of the vessel\n",
    "# apply a threshold: If value is within treshold, then no clipping needed\n",
    "#                    Else modify the value to the nearest value under the max capacity\n",
    "\n",
    "# level_readings = original_level_readings.copy()\n",
    "for vessel_id, vessel_group in level_readings.groupby(\"VESSEL_ID\"):\n",
    "    sorted_vessel_group = vessel_group.sort_values(by=[\"ON_DATE_TIME\"])\n",
    "    threshold = 400\n",
    "    max_capacity = customer_details.loc[np.where(vessel_id == customer_details[\"VESSEL_ID\"])[0], \"MAXIMUM_PRODUCT_CAPACITY\"].values\n",
    "    max_capacity = max_capacity + threshold\n",
    "    \n",
    "#     print(sorted_vessel_group[\"INST_PRODUCT_AMOUNT\"].values > 10)\n",
    "    \n",
    "    indices = np.where(sorted_vessel_group[\"INST_PRODUCT_AMOUNT\"].values > (max_capacity + threshold))[0]\n",
    "    to_clip_indices = sorted_vessel_group.index[indices]\n",
    "    \n",
    "    for clip_index in to_clip_indices:\n",
    "#         print(clip_index)\n",
    "#         print((clip_index-3 in sorted_vessel_group.index.values))\n",
    "#         print((clip_index-2 in sorted_vessel_group.index.values))\n",
    "#         print((clip_index-1 in sorted_vessel_group.index.values))\n",
    "        \n",
    "        if clip_index-3 in sorted_vessel_group.index.values or clip_index-2 in sorted_vessel_group.index.values or \\\n",
    "           clip_index-1 in sorted_vessel_group.index.values:\n",
    "            ind1 = clip_index - 1\n",
    "            ind2 = clip_index - 2\n",
    "            ind3 = clip_index - 3\n",
    "        else:\n",
    "            ind1 = clip_index + 1\n",
    "            ind2 = clip_index + 2\n",
    "            ind3 = clip_index + 3\n",
    "        \n",
    "#         print(clip_index == to_clip_indices[0])\n",
    "        smallest = np.array([ind1,ind2,ind3])\n",
    "        smallest_index = np.where(sorted_vessel_group.loc[clip_index, \"INST_PRODUCT_AMOUNT\"] > \\\n",
    "                    sorted_vessel_group.loc[smallest, \"INST_PRODUCT_AMOUNT\"] * 2)[0]\n",
    "        \n",
    "#         if not(smallest_distance):\n",
    "#             smallest_distance = np.argmin(sorted_vessel_group.loc[clip_index, \"INST_PRODUCT_AMOUNT\"] - \\\n",
    "#                     sorted_vessel_group.loc[[ind1,ind2,ind3], \"INST_PRODUCT_AMOUNT\"] * 1.5)\n",
    "        \n",
    "#             level_readings.loc[clip_index, \"INST_PRODUCT_AMOUNT\"] = level_readings.loc[smallest_distance, \"INST_PRODUCT_AMOUNT\"]\n",
    "# #             print(smallest_distance)\n",
    "#         else:\n",
    "#             print(smallest_distance)\n",
    "        if len(smallest_index) > 0 and str(smallest[smallest_index[0]]) != \"nan\":\n",
    "            find_smallest = np.argmin(smallest[smallest_index])\n",
    "            level_readings.loc[clip_index, \"INST_PRODUCT_AMOUNT\"] = level_readings.loc[find_smallest, \"INST_PRODUCT_AMOUNT\"]\n",
    "#         print(\"-\")\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_readings.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to csv  the data after clipping in Step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_readings.to_csv(\"after_clipping.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting delivery and level reading based on vessel id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vessel_id = deliveries[\"VESSEL_ID\"].unique()[np.random.randint(2347)]\n",
    "# vessel_id = \"CN-150090\"\n",
    "vessel_id = 'CN-136070'\n",
    "# vessel_id = 'BR-218596'\n",
    "vessel_id = 'BR-216705'\n",
    "vessel_id = \"CN-140691\"\n",
    "vessel_id = \"CN-136070\"\n",
    "vessel_id = \"CN-134330\"\n",
    "# vessel_id = \"TH-23561\"\n",
    "print(vessel_id)\n",
    "\n",
    "vessel_id_del_data = deliveries[deliveries[\"VESSEL_ID\"] == vessel_id].sort_values(by=[\"DELIVERY_DATE\"])\n",
    "vessel_id_level_data = level_readings[level_readings[\"VESSEL_ID\"] == vessel_id].sort_values(by=[\"ON_DATE_TIME\"])\n",
    "max_capacity = customer_details.loc[np.where(vessel_id == customer_details[\"VESSEL_ID\"])[0], \"MAXIMUM_PRODUCT_CAPACITY\"]\n",
    "# print(type(vessel_id_del_data[\"DELIVERY_DATE\"].values[0]))\n",
    "col_plot = column()\n",
    "\n",
    "p = figure(plot_width=970, plot_height=600, title=\"Deliveries for Vessel ID: \"+vessel_id, x_axis_type='datetime')\n",
    "\n",
    "p.vbar(x=vessel_id_del_data[\"DELIVERY_DATE\"], top=vessel_id_del_data[\"DELIVERED_VOLUME\"], width=100)\n",
    "# p.square(vessel_id_del_data[\"DELIVERY_DATE\"], vessel_id_del_data[\"DELIVERED_VOLUME\"], color='#A6CEE3', line_width=3)\n",
    "\n",
    "col_plot.children.append(p)\n",
    "\n",
    "p = figure(plot_width=970, plot_height=600, title=\"Level readings for Vessel ID: \"+vessel_id, x_axis_type='datetime')\n",
    "\n",
    "p.line(vessel_id_level_data[\"ON_DATE_TIME\"], vessel_id_level_data[\"INST_PRODUCT_AMOUNT\"], color='green', line_width=3)\n",
    "p.line(vessel_id_level_data[\"ON_DATE_TIME\"], np.ones(vessel_id_level_data[\"ON_DATE_TIME\"].shape[0]) * max_capacity.values, color='red', line_width=4)\n",
    "p.line(vessel_id_level_data[\"ON_DATE_TIME\"], np.ones(vessel_id_level_data[\"ON_DATE_TIME\"].shape[0]) * (max_capacity.values+threshold), color='purple', line_width=4)\n",
    "p.square(vessel_id_level_data[\"ON_DATE_TIME\"], vessel_id_level_data[\"INST_PRODUCT_AMOUNT\"], color='green', line_width=3)\n",
    "\n",
    "col_plot.children.append(p)\n",
    "\n",
    "show(col_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To check if all data points have unique timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for vessel_id in vessel_id_level_data[\"VESSEL_ID\"].unique():\n",
    "    # vessel_id_del_data = deliveries[deliveries[\"VESSEL_ID\"] == vessel_id].sort_values(by=[\"del_date\"])\n",
    "    vessel_id_level_data = level_readings[level_readings[\"VESSEL_ID\"] == vessel_id].sort_values(by=[\"read_date\"])\n",
    "    if len(vessel_id_level_data[\"ON_DATE_TIME\"].unique()) == vessel_id_level_data.shape[0]:\n",
    "        count = count + 1\n",
    "        \n",
    "if count == len(vessel_id_level_data[\"VESSEL_ID\"].unique()):\n",
    "    print(\"ALL UNIQUE\")\n",
    "else:\n",
    "    print(NOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_lvl_readings = pd.read_csv(\"/data/Linde_Intel_AI_Challenge_Nov2018/level_readings.csv\")\n",
    "\n",
    "updated_lvl_readings[\"read_date\"] = pd.to_datetime(updated_lvl_readings[\"ON_DATE_TIME\"]).dt.date\n",
    "updated_lvl_readings[\"read_time\"] = pd.to_datetime(updated_lvl_readings[\"ON_DATE_TIME\"]).dt.time\n",
    "updated_lvl_readings[\"ON_DATE_TIME\"] = pd.to_datetime(updated_lvl_readings[\"ON_DATE_TIME\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below -> Incomplete / Not used work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to clip all the sudden zeros or large distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_readings = original_level_readings.copy()\n",
    "\n",
    "for vessel_id, vessel_group in level_readings.groupby(\"VESSEL_ID\"):\n",
    "#     print(vessel_id)\n",
    "    \n",
    "    sorted_vessel_group = vessel_group.sort_values(by=[\"ON_DATE_TIME\"])\n",
    "    \n",
    "    current_minus = [t - s for s, t in zip(sorted_vessel_group[\"INST_PRODUCT_AMOUNT\"].values, sorted_vessel_group[\"INST_PRODUCT_AMOUNT\"].values[1:])]\n",
    "    next_minus = [t - s for s, t in zip(sorted_vessel_group[\"INST_PRODUCT_AMOUNT\"].values[1:], sorted_vessel_group[\"INST_PRODUCT_AMOUNT\"].values[2:])]\n",
    "    \n",
    "    minus_indices = sorted_vessel_group.index[1:][np.where(np.array(current_minus) > 100)[0]]\n",
    "#     print(len(minus_indices))\n",
    "    \n",
    "    next_indices = sorted_vessel_group.index[2:][np.where(np.array(next_minus) > 100)[0]]\n",
    "#     print(len(minus_indices))\n",
    "    \n",
    "    final_indices = np.intersect1d(minus_indices, next_indices)\n",
    "    final_indices = np.intersect1d(final_indices, sorted_vessel_group.index.values)\n",
    "    \n",
    "    for index in final_indices:\n",
    "        if index == sorted_vessel_group.index[-1]:\n",
    "            mean = (sorted_vessel_group.loc[index, \"INST_PRODUCT_AMOUNT\"] + sorted_vessel_group.loc[index-1, \"INST_PRODUCT_AMOUNT\"]) / 2\n",
    "        elif index+1 in sorted_vessel_group.index and index-1 in sorted_vessel_group.index:\n",
    "            mean = (sorted_vessel_group.loc[index+1, \"INST_PRODUCT_AMOUNT\"] + sorted_vessel_group.loc[index-1, \"INST_PRODUCT_AMOUNT\"]) / 2\n",
    "        else:\n",
    "            mean = sorted_vessel_group.loc[index, \"INST_PRODUCT_AMOUNT\"]/4\n",
    "            \n",
    "        level_readings.loc[index, \"INST_PRODUCT_AMOUNT\"] = mean\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# new level_reading dataframe\n",
    "updated_lvl_readings = level_readings.copy()\n",
    "indices_to_remove = []\n",
    "# group by vessel id\n",
    "for ves_id, vessel_group in level_readings.groupby(\"VESSEL_ID\"):\n",
    "    # sort the group\n",
    "    sorted_vessel_group = vessel_group.sort_values(by=[\"ON_DATE_TIME\"])\n",
    "    \n",
    "    # find max and min timestamps in the group\n",
    "    sorted_max = sorted_vessel_group[\"ON_DATE_TIME\"].max()\n",
    "    sorted_min = sorted_vessel_group[\"ON_DATE_TIME\"].min()\n",
    "    \n",
    "    # subtract the max from the timestamps of the entire group\n",
    "    # update in level readings\n",
    "#     updated_lvl_readings.loc[sorted_vessel_group.index, \"sub_time\"] = sorted_max - sorted_vessel_group[\"ON_DATE_TIME\"]\n",
    "    times_with_max = sorted_max - sorted_vessel_group[\"ON_DATE_TIME\"]\n",
    "    times_with_max = times_with_max / pd.Timedelta('1 hour')\n",
    "    times_with_max = times_with_max % 1\n",
    "    \n",
    "    times_with_min = sorted_vessel_group[\"ON_DATE_TIME\"] - sorted_min\n",
    "    times_with_min = times_with_min / pd.Timedelta('1 hour')\n",
    "    times_with_min = times_with_min % 1\n",
    "    \n",
    "    remove_min_indices = times_with_min.index[np.where(times_with_min > 0)[0]]\n",
    "    remove_max_indices = times_with_max.index[np.where(times_with_max > 0)[0]]\n",
    "    \n",
    "    indices_to_remove.append(np.unique(np.concatenate((remove_min_indices, remove_max_indices))).tolist())\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_lvl_readings.drop(index=sum(indices_to_remove, []), inplace=True)\n",
    "updated_lvl_readings.reset_index(drop=True, inplace=True)\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the modified csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colocations.to_csv(\"../data/colocations.csv\", sep=',')\n",
    "customer_details.to_csv(\"../data/customer_details.csv\", sep=',')\n",
    "deliveries.to_csv(\"../data/deliveries.csv\", sep=',')\n",
    "level_readings.to_csv(\"../data/level_readings.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
